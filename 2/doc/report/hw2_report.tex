%
% 6.006 problem set 5
%
\documentclass[12pt,twoside]{article}

\input{macros}

\usepackage{amsmath}
\usepackage{url}
\usepackage{mdwlist}
\usepackage{graphicx}
\usepackage{clrscode3e}
\newcommand{\isnotequal}{\mathrel{\scalebox{0.8}[1]{!}\hspace*{1pt}\scalebox{0.8}[1]{=}}}
\usepackage{listings}
\usepackage{tikz}
\usepackage{float}
\usetikzlibrary{arrows}
\usetikzlibrary{matrix}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{shapes.misc}
\usetikzlibrary{trees}
\usepackage{subfigure}

\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{caption}
\captionsetup{hypcap=true}

\newcommand{\answer}{
 \par\medskip
 \textbf{Answer:}
}

\newcommand{\collaborators}{ \textbf{Collaborators:}
%%% COLLABORATORS START %%%

\tabT Name: Zhuo Chenn

\tabT Student ID: 3170101214
%%% COLLABORATORS END %%%
}

\newcommand{\answerIa}{ \answer
%%% PROBLEM 1(a) ANSWER START %%%
\begin{enumerate}
	\item 10: 0\%, 10.98\%; 100: 0\%, 1.34\%.
	\item 10: 13; 100: 214.
	\item The program will not stop because the algorithm cannot converge.

\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{imgs/figure2}
	\caption{The plotting result for perceptron when nTrain = 100.}
	\label{figure2}
\end{figure}
%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.7]{figure3}
%	\caption{The plotting result for perceptron when training data is not linearly seperable.}
%	\label{figure3}
%\end{figure}
%%% PROBLEM 1(a) ANSWER END %%%
}

\newcommand{\answerIb}{ \answer
%%% PROBLEM 1(b) ANSWER START %%%
\begin{enumerate}
	\item The training error rate is 3.30\%, and test error rate is 3.96\%.
	\item 13.1\% for training set, 5.58\% for test set.
	\item 49\% for training set, 54.79\% for test set.
	\item 5\% for training set, 5.29\% for test set.
	
\end{enumerate}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{imgs/figure4}
	\caption{The plotting result for linear regression.}
	\label{figure4}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{imgs/figure5}
	\caption{The plotting result for linear regression when training data is not linearly seperable.}
	\label{figure5}
\end{figure}
%%% PROBLEM 1(b) ANSWER END %%%
}

\newcommand{\answerIc}{ \answer
	%%% PROBLEM 1(c) ANSWER START %%%
	\begin{enumerate}
		\item 7.52\% for training set, 8.55\% for test set.
		\item 17.1\% for training set, 10.8\% for test set.
	\end{enumerate}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{imgs/figure6}
		\caption{The plotting result for logistic regression.}
		\label{figure6}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{imgs/figure7}
		\caption{The plotting result for logistic regression when training data is not linearly seperable.}
		\label{figure7}
	\end{figure}
	%%% PROBLEM 1(c) ANSWER END %%%
}

\newcommand{\answerId}{ \answer
	%%% PROBLEM 1(d) ANSWER START %%%
	\begin{enumerate}
		\item 0\% for training set, 3.22\% for test set.
		\item 0\% for training set, 1.01\% for test set.
		\item 2.989.
		\item (bonus) When the training data is noisy and not linearly separable (nTrain = 100), SVM without slack variables has an avarage error rate of 47.5\% for training set and 47.6\% for test set, meanwhile SVM with slack variables (C = 4) has an avarage error rate of 13.1\% for training set and 5.48\% for test set.
	\end{enumerate}

	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{imgs/figure9}
		\caption{The plotting result for SVM when nTrain is 100.}
		\label{figure9}
	\end{figure}
	\begin{figure}
		\centering
		\subfigure[without slack variables]{
			\includegraphics[scale=0.5]{imgs/figure10}
		}\subfigure[with slack variables, C = 1]{
			\includegraphics[scale=0.5]{imgs/figure11}
		}
		\caption{The plotting result for SVM when training data is not linearly seperable.}
		\label{figure10}
	\end{figure}
	%%% PROBLEM 1(d) ANSWER END %%%
}

\newcommand{\answerIIa}{ \answer
%%% PROBLEM 2(a) ANSWER START %%%

\begin{enumerate}
	\item 100
	\item 201.78 for $\lambda=0$, 26.51 for $\lambda=100$
	\item 0\%/12.6\% for $\lambda=0$, 0\%/5.98\% for $\lambda=100$
\end{enumerate}
%%% PROBLEM 2(a) ANSWER END %%%
}


\newcommand{\answerIIb}{ \answer
%%% PROBLEM 2(b) ANSWER START %%%
0\%/6.58\% for $\lambda=0$, 0\%/5.02\% for $\lambda=1$. 
%%% PROBLEM 2(b) ANSWER END %%%
}



\newcommand{\answerIIIa}{ \answer 
%%% PROBLEM 3(a) ANSWER START %%%
\begin{enumerate}
	\item F
	\item F
	\item T
	\item F
	\item F
\end{enumerate}
%%% PROBLEM 3(a) ANSWER END %%%

}


\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}

% Fill these in!
\newcommand{\theproblemsetnum}{2}
\newcommand{\releasedate}{May 9, 2019}
\newcommand{\partaduedate}{Monsday, May 25}
\newcommand{\tabUnit}{3ex}
\newcommand{\tabT}{\hspace*{\tabUnit}}

\begin{document}

\handout{Homework \theproblemsetnum}{\releasedate}

%\textbf{Both theory and programming questions} are due {\bf \partaduedate} at
%{\bf 11:59PM}.
%
\collaborators
%Please download the .zip archive for this problem set, and refer to the
%\texttt{README.txt} file for instructions on preparing your solutions.
%
%We will provide the solutions to the problem set 10 hours after the problem set
%is due. You will have to read the solutions, and write a brief \textbf{grading
%explanation} to help your grader understand your write-up. You will need to
%submit the grading explanation by \textbf{Thursday, November 3rd, 11:59PM}. Your
%grade will be based on both your solutions and the grading explanation.

\medskip

\hrulefill

\begin{problems}

\problem \textbf{A Walk Through Linear Models}
\begin{problemparts}
\problempart Perceptron
\answerIa


\problempart Linear Regression
\answerIb

\problempart Logistic Regression

\answerIc


\problempart Support Vector Machine
\answerId


\end{problemparts}
\newpage

\problem \textbf{Regularization and Cross-Validation}
\begin{problemparts}
\problempart
Implement Ridge Regrssion, and use LOOCV to tune the regularization parameter $\lambda$.


\answerIIa

\problempart Implement Logistic Regrssion, and use LOOCV to tune the regularization parameter $\lambda$.

\answerIIb


\end{problemparts}

\problem \textbf{Bias Variance Trade-off}

Let's review the bias-variance decomposition first. Now please answer the following questions:
\begin {problemparts}
\problempart True of False

\answerIIIa


\end{problemparts}


\end{problems}
\end{document}
